from pyspark.sql import SparkSession
from pyspark.sql import DataFrame as SparkDataFrame
from pyspark.sql.functions import dayofmonth, year, count
from pyspark.sql.functions import dayofmonth, month, year
from pyspark.sql.functions import year, avg
from pyspark.sql.functions import year, stddev
from pyspark.sql.functions import col, sqrt, mean
jar_path = "/Users/vanand/Documents/projects/mysql-connector/mysql-connector-j-8.1.0.jar"
spark = SparkSession.builder.appName("pyspark-mysql").config("spark.jars", jar_path).getOrCreate()
mysql_props = {
    "driver": "com.mysql.cj.jdbc.Driver",
    "url": "jdbc:mysql://localhost:3306/mysql?useSSL=false&serverTimezone=UTC&allowPublicKeyRetrieval=true"
,
    "user": "root",
    "password": "_8Al@j!hsvigh_",
    "dbtable": "neic_earthquakes"
}
try:
    df = spark.read.format("jdbc").option("url", mysql_props["url"]).option("dbtable", mysql_props["dbtable"]).option("user", mysql_props["user"]).option("password", mysql_props["password"]).option("driver", mysql_props["driver"]).load()
except Exception as e:
    print(f"Error: {e}")
mysql_host = "localhost"
mysql_user = "root"
mysql_password = "_8Al@j!hsvigh_"
mysql_db = "mysql"
mysql_table = "neic_earthquakes"
import mysql.connector
import pandas as pd
connection = mysql.connector.connect(
    host=mysql_host,
    user=mysql_user,
    passwd=mysql_password,
    database=mysql_db
)
query = "SELECT * FROM neic_earthquakes"
data = pd.read_sql(query,connection)
data['Magnitude Type'].loc[data['Magnitude Type'].isnull()]
data = data.drop(data.index[[6703,7294,7919]])
data
data.info()
data.describe()
data.pivot_table(index = "Type", values = "Magnitude", aggfunc=len)
data.fillna(data.mean(), inplace=True)
data
data.pivot_table(index = "Type", values = "Magnitude", aggfunc=len)
data.describe()
data.head()
1. How does the Day of a Week affect the number of earthquakes? -----------------------------------
data['Date'] = pd.to_datetime(data['Date'])
if pd.api.types.is_datetime64_any_dtype(data['Date']):
    data['day_of_week'] = data['Date'].dt.dayofweek + 1 
    result = data.groupby("day_of_week").size().reset_index(name="earthquake_count")
data['Date'] = pd.to_datetime(data['Date'], utc = True)
result = data.groupby("Date").size().reset_index(name="earthquake_count").sort_values("Date")
result 
2. What is the relation between Day of the month and Number of earthquakes that
happened in a year? ------------------------------------------------------------------
spark_data2: SparkDataFrame = spark.createDataFrame(data)
earthquake_data = spark_data2.withColumn("year", year("date"))
result_2 = earthquake_data.groupBy("year", "date").count()
result_2.show()
3. What does the average frequency of earthquakes in a month from the year 1965 to 2016
tell us? ------------------------------------------------------
filtered_earthquakes = spark_data2.filter((year("date") >= 1965) & (year("date") <= 2016))
filtered_earthquakes = filtered_earthquakes.withColumn("day_of_month", dayofmonth("date"))
filtered_earthquakes = filtered_earthquakes.withColumn("month", month("Date"))
monthly_counts = filtered_earthquakes.groupBy("month").count()
monthly_counts.show()
# Calculate the average frequency of earthquakes per month
total_months = filtered_earthquakes.select("month").distinct().count()
average_frequency = monthly_counts.selectExpr("avg(count) as average_frequency").collect()[0]["average_frequency"]
average_frequency
4. What is the relation between Year and Number of earthquakes that happened in that
year?
year_data = spark_data2.withColumn("year", year("date"))
earthquakes_by_year = year_data.groupBy("year").agg(count("year").alias("earthquake_count"))
earthquakes_by_year.show()
5. How has the earthquake magnitude on average been varied over the years?
average_magnitude_by_year = year_data.groupBy("year").agg(avg("magnitude").alias("average_magnitude"))
average_magnitude_by_year.show()
6. How does year impact the standard deviation of the earthquakes?
stddev_by_year = year_data.groupBy("year").agg(stddev("magnitude").alias("magnitude_stddev"))
stddev_by_year.show()
7. Does geographic location have anything to do with earthquakes? -----------------------------------
result = earthquake_data.groupBy("Latitude", "Longitude").count()
result.show()
8. Where do earthquakes occur very frequently? 
if "Latitude" not in spark_data2.columns or "Longitude" not in spark_data2.columns:
    print("Invalid DataFrame. Please check column names.")
else:
    print("DataFrame Sample:")
    spark_data2.show(5, truncate=False)
if "Latitude" not in spark_data2.columns or "Longitude" not in spark_data2.columns:
    print("Invalid DataFrame. Please check column names.")
else:
    # Print DataFrame schema
    print("DataFrame Schema:")
    spark_data2.printSchema()

    # Aggregate earthquake counts based on latitude and longitude
    earthquake_frequency = spark_data2.groupBy("latitude", "longitude").count()

    # Show the result
    earthquake_frequency.show(truncate=False)
9. What is the relation between Magnitude, Magnitude Type , Status and Root Mean
Square of the earthquakes? -------------------------------------------------
data.head()
result = earthquake_data.groupBy("Magnitude Type", "Status").agg(
    mean("Magnitude").alias("Mean_Magnitude"),
    mean("Magnitude_RMS").alias("Mean_Magnitude_RMS")
)
result
result.show()
